{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UZ_G4XdfP7GK"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjTYqMoN8fh"
   },
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F55EwI6RQl1A",
    "outputId": "da1a2d29-4f6c-4c35-b810-e41e5b7df236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of original train set: 60000\n",
      "size of original test set: 20000\n",
      "****************************************************************************************************\n",
      "size of train set: 60000, #positive: 30055, #negative: 29945\n",
      "size of test set: 1000, #positive: 510, #negative: 490\n",
      "['it', 'will', 'help', 'relieve', 'your', 'stress', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken']\n",
      "sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "with open(\"./tweets_data/vocabulary.pkl\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_data = json.load(open('tweets_data/trainTweets_preprocessed.json', 'r'))\n",
    "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
    "train_tweets = np.array([t[0] for t in train_data])\n",
    "train_labels = np.array([int(t[1]) for t in train_data])\n",
    "\n",
    "test_data = json.load(open('tweets_data/testTweets_preprocessed.json', 'r'))\n",
    "test_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data))\n",
    "test_tweets = np.array([t[0] for t in test_data])\n",
    "test_labels = np.array([int(t[1]) for t in test_data])\n",
    "\n",
    "print(\"size of original train set: {}\".format(len(train_tweets)))\n",
    "print(\"size of original test set: {}\".format(len(test_tweets)))\n",
    "\n",
    "# only select first 1000 test sample for test\n",
    "test_tweets = test_tweets[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"size of train set: {}, #positive: {}, #negative: {}\".format(len(train_tweets), np.sum(train_labels), len(train_tweets)-np.sum(train_labels)))\n",
    "print(\"size of test set: {}, #positive: {}, #negative: {}\".format(len(test_tweets), np.sum(test_labels), len(test_tweets)-np.sum(test_labels)))\n",
    "\n",
    "# show text of the idx-th train tweet\n",
    "# The 'padtoken' is used to ensure each tweet has the same length\n",
    "idx = 100\n",
    "train_text = [vocabulary[x] for x in train_tweets[idx]]\n",
    "print(train_text)\n",
    "sentiment_label = [\"negative\", \"positive\"]\n",
    "print(\"sentiment: {}\".format(sentiment_label[train_labels[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmjy9sPDOCnY"
   },
   "source": [
    "## **Part 1 LSTM Encoder**\n",
    "\n",
    "The training data in the followind model receive inputs that have been one-hot encoded. To avoid unnecessary memory usage, only the batch sample is encoded during the training phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Mx6WgMBVI3T",
    "outputId": "f1b0080e-9bd8-4331-de6e-8babd6b2647c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 20, 7597)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                1961472   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,963,618\n",
      "Trainable params: 1,963,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "235/235 [==============================] - 21s 79ms/step - loss: 0.5624 - accuracy: 0.6999 - val_loss: 0.5035 - val_accuracy: 0.7530\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.4576 - accuracy: 0.7837 - val_loss: 0.4947 - val_accuracy: 0.7670\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.4280 - accuracy: 0.8011 - val_loss: 0.5017 - val_accuracy: 0.7570\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.3982 - accuracy: 0.8179 - val_loss: 0.5265 - val_accuracy: 0.7580\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 18s 79ms/step - loss: 0.3701 - accuracy: 0.8307 - val_loss: 0.5445 - val_accuracy: 0.7470\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.3415 - accuracy: 0.8458 - val_loss: 0.5787 - val_accuracy: 0.7440\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 18s 79ms/step - loss: 0.3142 - accuracy: 0.8606 - val_loss: 0.5561 - val_accuracy: 0.7530\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 18s 79ms/step - loss: 0.2845 - accuracy: 0.8745 - val_loss: 0.7179 - val_accuracy: 0.7480\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.2587 - accuracy: 0.8879 - val_loss: 0.7555 - val_accuracy: 0.7530\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.2475 - accuracy: 0.8923 - val_loss: 0.8614 - val_accuracy: 0.7420\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.2242 - accuracy: 0.9027 - val_loss: 0.9263 - val_accuracy: 0.7440\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.2032 - accuracy: 0.9116 - val_loss: 1.0539 - val_accuracy: 0.7450\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 19s 80ms/step - loss: 0.1898 - accuracy: 0.9186 - val_loss: 1.2472 - val_accuracy: 0.7370\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.1775 - accuracy: 0.9233 - val_loss: 1.2420 - val_accuracy: 0.7330\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.1627 - accuracy: 0.9301 - val_loss: 1.5615 - val_accuracy: 0.7330\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.1503 - accuracy: 0.9359 - val_loss: 1.4673 - val_accuracy: 0.7450\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.1466 - accuracy: 0.9377 - val_loss: 1.5961 - val_accuracy: 0.7410\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.1335 - accuracy: 0.9424 - val_loss: 1.8064 - val_accuracy: 0.7420\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 18s 79ms/step - loss: 0.1253 - accuracy: 0.9460 - val_loss: 1.7217 - val_accuracy: 0.7560\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 0.1184 - accuracy: 0.9490 - val_loss: 1.6213 - val_accuracy: 0.7330\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Lambda\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Lambda(lambda x: tf.one_hot(tf.cast(x, dtype='int32'), len(vocabulary)),output_shape= (20,len(vocabulary)), input_shape=(20,)))\n",
    "model.add(LSTM(64, input_shape=(20,7597), activation='relu', return_sequences=False))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history_LSTM = model.fit(train_tweets, train_labels, epochs=20, batch_size=256,validation_data=(test_tweets,test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBiLRGc7RL-g"
   },
   "source": [
    "## **Part 2: Embedding Lookup layer**\n",
    "\n",
    "Word embedding: Instead of using a one-hot vector to represent each word, we can add an word embedding matrix in which each word is represented as a low-dimensional vector. This representation is not sparse any more, because we're working in a continuous vector space now. Words that share similar/related semantic meaning should be 'close to each other' in this vector space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2mkQlVMVUny",
    "outputId": "01bf6115-343f-4672-b07b-5894966415c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.6931\n",
      "Training loss (for one batch) at step 400: 0.5351\n",
      "Training loss (for one batch) at step 800: 0.5044\n",
      "Training acc over epoch: 0.7262\n",
      "Validation acc: 0.7690\n",
      "Time taken: 87.09s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.4460\n",
      "Training loss (for one batch) at step 400: 0.4601\n",
      "Training loss (for one batch) at step 800: 0.5267\n",
      "Training acc over epoch: 0.7892\n",
      "Validation acc: 0.7670\n",
      "Time taken: 87.72s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.4015\n",
      "Training loss (for one batch) at step 400: 0.3827\n",
      "Training loss (for one batch) at step 800: 0.3712\n",
      "Training acc over epoch: 0.8075\n",
      "Validation acc: 0.7670\n",
      "Time taken: 86.89s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.3565\n",
      "Training loss (for one batch) at step 400: 0.3220\n",
      "Training loss (for one batch) at step 800: 0.3188\n",
      "Training acc over epoch: 0.8217\n",
      "Validation acc: 0.7630\n",
      "Time taken: 87.36s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.3810\n",
      "Training loss (for one batch) at step 400: 0.2952\n",
      "Training loss (for one batch) at step 800: 0.4129\n",
      "Training acc over epoch: 0.8358\n",
      "Validation acc: 0.7600\n",
      "Time taken: 87.03s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.4355\n",
      "Training loss (for one batch) at step 400: 0.2592\n",
      "Training loss (for one batch) at step 800: 0.3486\n",
      "Training acc over epoch: 0.8483\n",
      "Validation acc: 0.7650\n",
      "Time taken: 86.77s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.2919\n",
      "Training loss (for one batch) at step 400: 0.3464\n",
      "Training loss (for one batch) at step 800: 0.2362\n",
      "Training acc over epoch: 0.8597\n",
      "Validation acc: 0.7550\n",
      "Time taken: 87.21s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.3263\n",
      "Training loss (for one batch) at step 400: 0.2009\n",
      "Training loss (for one batch) at step 800: 0.3518\n",
      "Training acc over epoch: 0.8701\n",
      "Validation acc: 0.7460\n",
      "Time taken: 87.03s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.2823\n",
      "Training loss (for one batch) at step 400: 0.2391\n",
      "Training loss (for one batch) at step 800: 0.3104\n",
      "Training acc over epoch: 0.8793\n",
      "Validation acc: 0.7390\n",
      "Time taken: 87.03s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.2697\n",
      "Training loss (for one batch) at step 400: 0.2209\n",
      "Training loss (for one batch) at step 800: 0.2291\n",
      "Training acc over epoch: 0.8900\n",
      "Validation acc: 0.7440\n",
      "Time taken: 87.52s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.2034\n",
      "Training loss (for one batch) at step 400: 0.1724\n",
      "Training loss (for one batch) at step 800: 0.1578\n",
      "Training acc over epoch: 0.8970\n",
      "Validation acc: 0.7430\n",
      "Time taken: 87.24s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.1766\n",
      "Training loss (for one batch) at step 400: 0.1491\n",
      "Training loss (for one batch) at step 800: 0.1718\n",
      "Training acc over epoch: 0.9036\n",
      "Validation acc: 0.7390\n",
      "Time taken: 86.77s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.2638\n",
      "Training loss (for one batch) at step 400: 0.1455\n",
      "Training loss (for one batch) at step 800: 0.1575\n",
      "Training acc over epoch: 0.9107\n",
      "Validation acc: 0.7320\n",
      "Time taken: 87.55s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.1568\n",
      "Training loss (for one batch) at step 400: 0.1947\n",
      "Training loss (for one batch) at step 800: 0.2850\n",
      "Training acc over epoch: 0.9175\n",
      "Validation acc: 0.7410\n",
      "Time taken: 87.21s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.0761\n",
      "Training loss (for one batch) at step 400: 0.1073\n",
      "Training loss (for one batch) at step 800: 0.1084\n",
      "Training acc over epoch: 0.9231\n",
      "Validation acc: 0.7330\n",
      "Time taken: 86.92s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.1299\n",
      "Training loss (for one batch) at step 400: 0.2316\n",
      "Training loss (for one batch) at step 800: 0.1369\n",
      "Training acc over epoch: 0.9280\n",
      "Validation acc: 0.7410\n",
      "Time taken: 87.26s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.1611\n",
      "Training loss (for one batch) at step 400: 0.2387\n",
      "Training loss (for one batch) at step 800: 0.2026\n",
      "Training acc over epoch: 0.9319\n",
      "Validation acc: 0.7310\n",
      "Time taken: 87.27s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.2516\n",
      "Training loss (for one batch) at step 400: 0.1482\n",
      "Training loss (for one batch) at step 800: 0.1615\n",
      "Training acc over epoch: 0.9372\n",
      "Validation acc: 0.7390\n",
      "Time taken: 87.02s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.0942\n",
      "Training loss (for one batch) at step 400: 0.1497\n",
      "Training loss (for one batch) at step 800: 0.2111\n",
      "Training acc over epoch: 0.9398\n",
      "Validation acc: 0.7370\n",
      "Time taken: 86.48s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.2314\n",
      "Training loss (for one batch) at step 400: 0.1599\n",
      "Training loss (for one batch) at step 800: 0.0332\n",
      "Training acc over epoch: 0.9409\n",
      "Validation acc: 0.7340\n",
      "Time taken: 85.82s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(20,), name=\"digits\")\n",
    "x1 = Embedding(60000, 64, input_length=20)(inputs)\n",
    "x2 = LSTM(64, input_shape=(20,64), activation='relu', return_sequences=False)(x1)\n",
    "x3 = Dense(32,activation='relu')(x2)\n",
    "outputs = Dense(2,activation='softmax',name =\"predictions\")(x3)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_tweets,train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test_tweets, test_labels))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = model(x_batch_train, training=True)\n",
    "        loss_value = loss_fn(y_batch_train, logits)\n",
    "      grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "      train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "      if step % 400 == 0:\n",
    "          print(\n",
    "              \"Training loss (for one batch) at step %d: %.4f\"\n",
    "              % (step, float(loss_value))\n",
    "          )\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
